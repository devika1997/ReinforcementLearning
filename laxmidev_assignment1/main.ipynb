{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cdbddf5",
      "metadata": {
        "id": "8cdbddf5"
      },
      "source": [
        "I certify that the code and data in this assignment were generated independently, using only the tools\n",
        "and resources defined in the course and that I did not receive any external help, coaching or contributions\n",
        "during the production of this work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18M8q5ot-vY0",
      "metadata": {
        "id": "18M8q5ot-vY0"
      },
      "source": [
        "  **PART 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6e16b1",
      "metadata": {
        "id": "bd6e16b1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym import spaces\n",
        "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,AnnotationBbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "51e7f501",
      "metadata": {
        "id": "51e7f501"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'gym' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20972/696633404.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mGridEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ],
      "source": [
        "class GridEnvironment(gym.Env):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.observation_space = spaces.Discrete(16)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.timestep = 0\n",
        "        self.max_timesteps = 100\n",
        "        \n",
        "        self.max_episodes=1000\n",
        "        self.max_epsi=1\n",
        "        self.min_epsi=0.01\n",
        "        self.decayfactor=0.01\n",
        "        self.alpha=0.14\n",
        "        self.gamma=0.6\n",
        "        \n",
        "    def reset(self):\n",
        "        self.timestep = 0\n",
        "\n",
        "        #count\n",
        "        self.carrotcount=2\n",
        "        self.hurdlecount=1\n",
        "        self.trapcount=1\n",
        "\n",
        "        #positions\n",
        "        self.rabbit_pos = [0, 0]\n",
        "        self.goal = [3, 3]\n",
        "        self.trap= [1,1]\n",
        "        self.hurdle=[3,2]\n",
        "        self.carrot1=[1,3]\n",
        "        self.carrot2=[2,0]\n",
        "        self.carrot1_collect=False\n",
        "        self.carrot2_collect=False\n",
        "        self.hurdlegotrabbit=False\n",
        "        self.trapgotrabbit=False\n",
        "    \n",
        "    def step(self, action,choice):\n",
        "     \n",
        "      done = False\n",
        "      self.hurdlegotrabbit=False\n",
        "      self.trapgotrabbit=False\n",
        "      '''\n",
        "        Defining the Stochastic environment\n",
        "\n",
        "        Randomly selecting a probability between 0.1 and 0.9\n",
        "        if it is greater than or equal to 0.8 choosing a new action leaving the choosen action\n",
        "      '''\n",
        "\n",
        "      if choice == 0:\n",
        "        prob=random.randint(1,9)/10\n",
        "        if(prob>=0.8):\n",
        "          start=0\n",
        "          end=4\n",
        "          a=list(range(start,action))+list(range(action+1,end))\n",
        "          action=random.choice(a)\n",
        "\n",
        "      \"\"\"\n",
        "        action=0 up\n",
        "        action=1 down\n",
        "        action=2 left\n",
        "        action=3 right\n",
        "      \"\"\"  \n",
        "  \n",
        "      if action == 0:\n",
        "        self.rabbit_pos[1] += 1\n",
        "      if action == 1:\n",
        "        self.rabbit_pos[1] -= 1\n",
        "      if action == 2:\n",
        "        self.rabbit_pos[0] -= 1\n",
        "      if action == 3:\n",
        "        self.rabbit_pos[0] += 1\n",
        "\n",
        "      #Clipping the position so that the agent doesn't go out of the grid\n",
        "      self.rabbit_pos = np.clip(self.rabbit_pos, 0, 3)\n",
        "      \n",
        "      reward = 0\n",
        "      if (self.rabbit_pos == self.goal).all():\n",
        "        reward = 2\n",
        "        done=True\n",
        "      elif (self.rabbit_pos == self.trap).all() and self.trapgotrabbit==False and self.trapcount>0:\n",
        "        reward = -1\n",
        "        self.trapgotrabbit=True\n",
        "        self.trapcount -=1\n",
        "      elif (self.rabbit_pos == self.hurdle).all() and self.hurdlegotrabbit==False and self.hurdlecount>0:\n",
        "        reward = -0.5\n",
        "        self.hurdlegotrabbit=True\n",
        "        self.hurdlecount -=1\n",
        "      elif (self.rabbit_pos == self.carrot1).all() and self.carrot1_collect==False and self.carrotcount>0:\n",
        "        self.carrot1_collect=True\n",
        "        reward = 1\n",
        "        self.carrotcount -=1\n",
        "      elif (self.rabbit_pos == self.carrot2).all() and self.carrot2_collect==False and self.carrotcount>0:\n",
        "        self.carrot2_collect=True\n",
        "        reward = 1\n",
        "        self.carrotcount -=1\n",
        "         \n",
        "      self.timestep += 1\n",
        "      done = True if self.timestep >= self.max_timesteps or done==True else False\n",
        "      return reward, done\n",
        "        \n",
        "    def render(self):\n",
        "      fig,ax=plt.subplots(figsize=(10,10))\n",
        "      ax.set_xlim(0,4)\n",
        "      ax.set_ylim(0,4)\n",
        "\n",
        "      def plot_image(plot_pos):\n",
        "        \n",
        "        plot_rabbit,plot_hurdle,plot_trap,plot_carrot1,plot_carrot2,plot_goal=False,False,False,False,False,False\n",
        "\n",
        "        if (self.rabbit_pos == plot_pos).all():\n",
        "          plot_rabbit=True\n",
        "        if (self.hurdle == plot_pos).all() :\n",
        "          plot_hurdle=True\n",
        "        if (self.trap== plot_pos).all() :\n",
        "          plot_trap=True\n",
        "        if (self.carrot1 == plot_pos).all() and self.carrot1_collect==False:\n",
        "          plot_carrot1=True\n",
        "        if (self.carrot2 == plot_pos).all() and self.carrot2_collect==False:\n",
        "          plot_carrot2=True\n",
        "        if (self.goal == plot_pos).all():\n",
        "          plot_goal=True\n",
        "\n",
        "        #plot for rabbit\n",
        "        if plot_rabbit and all(not item for item in [plot_hurdle,plot_goal,plot_carrot1,plot_carrot2,plot_trap]):\n",
        "          rabbit=AnnotationBbox(OffsetImage(plt.imread('./images/rabbit.jpg'),zoom=0.28),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(rabbit)\n",
        "\n",
        "        #plot for trap\n",
        "        elif plot_trap and self.trapcount>0 and all(not item for item in [plot_rabbit,plot_hurdle,plot_goal,plot_carrot1,plot_carrot2]):\n",
        "          trap=AnnotationBbox(OffsetImage(plt.imread('./images/trap.jpg'),zoom=0.2),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(trap)\n",
        "        \n",
        "        #plot for hurdle\n",
        "        elif plot_hurdle and self.hurdlecount>0 and all(not item for item in [plot_rabbit,plot_trap,plot_goal,plot_carrot1,plot_carrot2]):\n",
        "          hurdle=AnnotationBbox(OffsetImage(plt.imread('./images/hurdle.jpg'),zoom=0.37),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(hurdle)\n",
        "        \n",
        "        #plot for carrot1\n",
        "        elif plot_carrot1  and all(not item for item in [plot_rabbit,plot_hurdle,plot_goal,plot_carrot2,plot_trap]):\n",
        "          carrot=AnnotationBbox(OffsetImage(plt.imread('./images/carrot.png'),zoom=0.12),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(carrot)\n",
        "        \n",
        "        #plot for carrot2\n",
        "        elif plot_carrot2 and all(not item for item in [plot_rabbit,plot_hurdle,plot_goal,plot_carrot1,plot_trap]):\n",
        "          carrot=AnnotationBbox(OffsetImage(plt.imread('./images/carrot.png'),zoom=0.12),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(carrot)\n",
        "        \n",
        "        #plot for goal\n",
        "        elif plot_goal and all(not item for item in [plot_rabbit,plot_hurdle,plot_carrot2,plot_trap,plot_carrot1]):\n",
        "          goal=AnnotationBbox(OffsetImage(plt.imread('./images/goal.jpg'),zoom=0.25),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(goal)\n",
        "\n",
        "        #plot for rabbit getting trapped\n",
        "        elif all(item for item in[plot_rabbit,plot_trap]) and self.trapgotrabbit==True and all(not item for item in [plot_hurdle,plot_goal,plot_carrot1,plot_carrot2]):\n",
        "          rabbit_trap=AnnotationBbox(OffsetImage(plt.imread('./images/rabbit_trap.png'),zoom=0.7),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(rabbit_trap)\n",
        "        elif all(item for item in[plot_rabbit,plot_trap]) and self.trapgotrabbit==False and all(not item for item in [plot_hurdle,plot_goal,plot_carrot1,plot_carrot2]):\n",
        "          rabbit=AnnotationBbox(OffsetImage(plt.imread('./images/rabbit.jpg'),zoom=0.28),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(rabbit)\n",
        "\n",
        "        #plot for rabbit got to a hurdle\n",
        "        elif all(item for item in[plot_rabbit,plot_hurdle]) and self.hurdlegotrabbit==True and all(not item for item in [plot_trap,plot_goal,plot_carrot1,plot_carrot2]):\n",
        "          rabbit_hurdle=AnnotationBbox(OffsetImage(plt.imread('./images/rabbit_hurdle.jpeg'),zoom=0.1),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(rabbit_hurdle)\n",
        "        elif all(item for item in[plot_rabbit,plot_hurdle]) and self.hurdlegotrabbit==False and all(not item for item in [plot_trap,plot_goal,plot_carrot1,plot_carrot2]):\n",
        "          rabbit=AnnotationBbox(OffsetImage(plt.imread('./images/rabbit.jpg'),zoom=0.28),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(rabbit)\n",
        "\n",
        "        #rabbit reaching the goal\n",
        "        elif all(item for item in[plot_rabbit,plot_goal]) and all(not item for item in [plot_trap,plot_hurdle,plot_carrot1,plot_carrot2]):\n",
        "          rabbit_goal=AnnotationBbox(OffsetImage(plt.imread('./images/rabbit_goal.png'),zoom=0.4),np.add(plot_pos,[0.5,0.5]),frameon=False)\n",
        "          ax.add_artist(rabbit_goal)\n",
        "    \n",
        "      state_mapping={}\n",
        "      for j in range(16):\n",
        "        state_mapping[j]=np.asarray([j%4,int(np.floor(j/4))])\n",
        "\n",
        "      for position in state_mapping:\n",
        "        plot_image(state_mapping[position])\n",
        "    \n",
        "      plt.xticks([0,1,2,3,4])\n",
        "      plt.yticks([0,1,2,3,4])\n",
        "      plt.grid()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5ca16e",
      "metadata": {
        "id": "1e5ca16e"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.observation_space = env.observation_space\n",
        "    self.action_space = env.action_space\n",
        "\n",
        "  def step(self, observation):\n",
        "    return np.random.choice(self.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9612c8",
      "metadata": {
        "id": "4f9612c8"
      },
      "outputs": [],
      "source": [
        "env = GridEnvironment()\n",
        "agent = RandomAgent(env)\n",
        "\n",
        "env.reset()\n",
        "env.max_timesteps = 10\n",
        "done = False\n",
        "while not done:\n",
        "  action = agent.step(env)\n",
        "  \"\"\"\n",
        "  choice = 0 Stochastic\n",
        "  choice = 1 Deterministic\n",
        "  \"\"\"\n",
        "  reward, done = env.step(action,choice=1)\n",
        "  env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pnpiA30zAqCV",
      "metadata": {
        "id": "pnpiA30zAqCV"
      },
      "source": [
        "**e-greedy algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ad9ecb",
      "metadata": {
        "id": "92ad9ecb"
      },
      "outputs": [],
      "source": [
        "def e_greedy(Q, state,nactions , epsi):\n",
        "    #Exploration\n",
        "    if random.uniform(0, 1)  < epsi:\n",
        "        action = np.random.randint(0, nactions)\n",
        "\n",
        "    #Exploitation\n",
        "    else:\n",
        "        val = max(Q[tuple(state)].values())\n",
        "        action = np.random.choice([i for i, j in Q[tuple(state)].items() if j == val])\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91wb3Tb9Awnu",
      "metadata": {
        "id": "91wb3Tb9Awnu"
      },
      "source": [
        "**Q Learning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69adebeb",
      "metadata": {
        "id": "69adebeb"
      },
      "outputs": [],
      "source": [
        "def qlearning(env,choice):\n",
        "    rewards_episode = []  \n",
        "    epsilons = []\n",
        "    Q = dict()\n",
        "    for row in range(4):\n",
        "        for column in range(4):\n",
        "                Q[(row,column)] = {0:0, 1:0, 2:0, 3:0}\n",
        "    epsilon=1\n",
        "    for i in range(env.max_episodes):\n",
        "        cummu_reward=0\n",
        "        env.reset()\n",
        "        done=False\n",
        "        count=0\n",
        "        while not done:\n",
        "            old_state = env.rabbit_pos.copy()\n",
        "            action=e_greedy(Q,old_state,env.action_space.n,epsilon)\n",
        "            #print(action)\n",
        "            #print(env.rabbit_pos)\n",
        "            reward, done= env.step(action)\n",
        "            #print(Q[tuple(env.rabbit_pos)].values())\n",
        "            new_state=env.rabbit_pos\n",
        "            Q[tuple(old_state)][action] += env.alpha * (reward + env.gamma * max(Q[tuple(new_state)].values()) - Q[tuple(old_state)][action])\n",
        "            cummu_reward+=reward\n",
        "            old_state=env.rabbit_pos.copy()\n",
        "            count+=1\n",
        "        rewards_episode.append(cummu_reward)\n",
        "        epsilon = env.min_epsi + (env.max_epsi - env.min_epsi)*np.exp(-env.decayfactor*i)\n",
        "        epsilons.append(epsilon)\n",
        "        #print(count)\n",
        "    return Q,rewards_episode,epsilons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UJFC5-3iKW72",
      "metadata": {
        "id": "UJFC5-3iKW72"
      },
      "source": [
        "**Deterministic Qlearning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0db71e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "3f0db71e",
        "outputId": "aabf96a3-ab00-432c-d54a-e86a2c1b5109"
      },
      "outputs": [],
      "source": [
        "#Deterministic Q-Learning\n",
        "env = GridEnvironment()\n",
        "env.reset()\n",
        "optimalQ,rewardsepisode,epsilons=qlearning(env,choice=1)\n",
        "\n",
        "#Plotting\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "x = range(len(rewardsepisode))\n",
        "plt.plot(x,rewardsepisode)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "x1= range(len(epsilons))\n",
        "plt.plot(x1,epsilons)\n",
        "plt.title('Epsilon decay')\n",
        "plt.ylabel('Epsilons')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48c9f04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c48c9f04",
        "outputId": "551cdb16-82d1-4106-c735-2a4a951a57a2"
      },
      "outputs": [],
      "source": [
        "env=GridEnvironment()\n",
        "epsireward=[]\n",
        "for i in range(1,11):\n",
        "  env.reset()\n",
        "  done = False\n",
        "  r=0\n",
        "  env.max_timesteps=10\n",
        "  print('EPISODE:',i)\n",
        "  while not done:\n",
        "    val = max(optimalQ[tuple(env.rabbit_pos)].values())\n",
        "    actions=optimalQ[tuple(env.rabbit_pos)]\n",
        "    action=max(actions,key=actions.get)\n",
        "    \"\"\"\n",
        "    choice = 0 Stochastic\n",
        "    choice = 1 Deterministic\n",
        "    \"\"\"\n",
        "    reward, done = env.step(action,choice=1)\n",
        "    r+=reward\n",
        "    env.render()\n",
        "  epsireward.append(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UaPseUxckjlz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "UaPseUxckjlz",
        "outputId": "7bc5d776-a10e-4126-ce2c-b4c47df20e1e"
      },
      "outputs": [],
      "source": [
        "x = range(len(epsireward))\n",
        "plt.plot(x,epsireward)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32S8kNhzKWyR",
      "metadata": {
        "id": "32S8kNhzKWyR"
      },
      "source": [
        "**Stochastic Qlearning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b470b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "02b470b0",
        "outputId": "a5f4b33c-ac8a-4f56-cd3f-6fd6879336c7"
      },
      "outputs": [],
      "source": [
        "#Stochastic Q-Learning\n",
        "env = GridEnvironment()\n",
        "env.reset()\n",
        "optimalQ,rewardsepisode,epsilons=qlearning(env,choice=0)\n",
        "\n",
        "#Plotting\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "x = range(len(rewardsepisode))\n",
        "plt.plot(x,rewardsepisode)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "x1= range(len(epsilons))\n",
        "plt.plot(x1,epsilons)\n",
        "plt.title('Epsilon decay')\n",
        "plt.ylabel('Epsilons')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15a774f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b15a774f",
        "outputId": "daac4d38-a3af-4865-e60d-7bf219c41620"
      },
      "outputs": [],
      "source": [
        "env=GridEnvironment()\n",
        "sqrewards=[]\n",
        "doneq=[]\n",
        "for i in range(1,11):\n",
        "  print('EPISODE:',i)\n",
        "  env.reset()\n",
        "  done = False\n",
        "  env.max_timesteps=12\n",
        "  rs=0\n",
        "  while not done:\n",
        "    val = max(optimalQ[tuple(env.rabbit_pos)].values())\n",
        "    actions=optimalQ[tuple(env.rabbit_pos)]\n",
        "    action=max(actions,key=actions.get)\n",
        "    #action = np.random.choice([i for i, j in optimalQ[tuple(env.rabbit_pos)].items() if j == val])\n",
        "    \"\"\"\n",
        "    choice = 0 Stochastic\n",
        "    choice = 1 Deterministic\n",
        "    \"\"\"\n",
        "    reward, done = env.step(action,choice=0)\n",
        "    rs+=reward\n",
        "    env.render()\n",
        "  sqrewards.append(rs)\n",
        "  doneq.append(done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X8xHa57NowYy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "X8xHa57NowYy",
        "outputId": "61998065-23ea-4aeb-d808-2c4b5c098370"
      },
      "outputs": [],
      "source": [
        "x = range(len(sqrewards))\n",
        "plt.plot(x,sqrewards)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IbgPVgqEKkpR",
      "metadata": {
        "id": "IbgPVgqEKkpR"
      },
      "source": [
        "**SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76071318",
      "metadata": {
        "id": "76071318"
      },
      "outputs": [],
      "source": [
        "def sarsa(env,choice):\n",
        "    rewards_episode = []  \n",
        "    epsilons = []\n",
        "    Qs = dict()\n",
        "    for row in range(4):\n",
        "        for column in range(4):\n",
        "                Qs[(row,column)] = {0:0, 1:0, 2:0, 3:0}\n",
        "    epsilon=1\n",
        "    for i in range(env.max_episodes):\n",
        "        cummu_reward=0\n",
        "        env.reset()\n",
        "        done=False\n",
        "        count=0\n",
        "        old_state = env.rabbit_pos.copy()\n",
        "        action=e_greedy(Qs,old_state,env.action_space.n,epsilon)\n",
        "        while not done:\n",
        "            reward, done= env.step(action,choice)\n",
        "            new_state=env.rabbit_pos.copy()\n",
        "            new_action=e_greedy(Qs,new_state,env.action_space.n,epsilon)\n",
        "            Qs[tuple(old_state)][action] += env.alpha * (reward + env.gamma * Qs[tuple(new_state)][new_action] - Qs[tuple(old_state)][action])\n",
        "            cummu_reward+=reward\n",
        "            old_state=env.rabbit_pos.copy()\n",
        "            action=new_action\n",
        "            count+=1\n",
        "        rewards_episode.append(cummu_reward)\n",
        "        epsilon = env.min_epsi + (env.max_epsi - env.min_epsi)*np.exp(-env.decayfactor*i)\n",
        "        epsilons.append(epsilon)\n",
        "        #print(count)\n",
        "    #print(rewards_episode)\n",
        "    return Qs,rewards_episode,epsilons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "as48OGWrKubh",
      "metadata": {
        "id": "as48OGWrKubh"
      },
      "source": [
        "**Deterministic SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e7fc37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "56e7fc37",
        "outputId": "adda8535-fd0e-4a7e-f841-1d077285792f"
      },
      "outputs": [],
      "source": [
        "#Deterministic SARSA\n",
        "env = GridEnvironment()\n",
        "env.reset()\n",
        "optimalQs,sdrewards,sdepsilons=sarsa(env,choice=1)\n",
        "\n",
        "#Plotting\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "x = range(len(sdrewards))\n",
        "plt.plot(x,sdrewards)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "x1= range(len(sdepsilons))\n",
        "plt.plot(x1,sdepsilons)\n",
        "plt.title('Epsilon decay')\n",
        "plt.ylabel('Epsilons')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130919c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "130919c9",
        "outputId": "a700c8b5-e3ec-42ca-c36f-11310d9ba622"
      },
      "outputs": [],
      "source": [
        "env=GridEnvironment()\n",
        "sdrew=[]\n",
        "for i in range(1,11):\n",
        "  print('EPISODE:',i)\n",
        "  env.reset()\n",
        "  done = False\n",
        "  env.max_timesteps=10\n",
        "  sdr=0\n",
        "  while not done:\n",
        "    val = max(optimalQs[tuple(env.rabbit_pos)].values())\n",
        "    actions=optimalQs[tuple(env.rabbit_pos)]\n",
        "    action=max(actions,key=actions.get)\n",
        "    \"\"\"\n",
        "    choice = 0 Stochastic\n",
        "    choice = 1 Deterministic\n",
        "    \"\"\"\n",
        "    reward, done = env.step(action,choice=1)\n",
        "    sdr+=reward\n",
        "    env.render()\n",
        "  sdrew.append(sdr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IitLsdRarer3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "IitLsdRarer3",
        "outputId": "d0b66754-0217-4948-9adb-eb28abf6a792"
      },
      "outputs": [],
      "source": [
        "x = range(len(sdrew))\n",
        "plt.plot(x,sdrew)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j5Jxe4eIK0CD",
      "metadata": {
        "id": "j5Jxe4eIK0CD"
      },
      "source": [
        "**Stochastic SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4bd7501",
      "metadata": {
        "id": "d4bd7501"
      },
      "outputs": [],
      "source": [
        "#Stochastic SARSA\n",
        "env = GridEnvironment()\n",
        "env.reset()\n",
        "optimalQss,ssrewards,ssepsilons=sarsa(env,choice=0)\n",
        "\n",
        "#Plotting\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "x = range(len(ssrewards))\n",
        "plt.plot(x,ssrewards)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "x1= range(len(ssepsilons))\n",
        "plt.plot(x1,ssepsilons)\n",
        "plt.title('Epsilon decay')\n",
        "plt.ylabel('Epsilons')\n",
        "plt.xlabel('Episodes')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CZAqWa0irvOs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CZAqWa0irvOs",
        "outputId": "a0accfdb-57ef-4bd7-8187-66e514880857"
      },
      "outputs": [],
      "source": [
        "env=GridEnvironment()\n",
        "ssrew=[]\n",
        "dones=[]\n",
        "for i in range(1,11):\n",
        "  print('EPISODE:',i)\n",
        "  env.reset()\n",
        "  done = False\n",
        "  env.max_timesteps=12\n",
        "  ssr=0\n",
        "  while not done:\n",
        "    val = max(optimalQss[tuple(env.rabbit_pos)].values())\n",
        "    actions=optimalQss[tuple(env.rabbit_pos)]\n",
        "    action=max(actions,key=actions.get)\n",
        "    \"\"\"\n",
        "    choice = 0 Stochastic\n",
        "    choice = 1 Deterministic\n",
        "    \"\"\"\n",
        "    reward, done = env.step(action,choice=0)\n",
        "    ssr+=reward\n",
        "    env.render()\n",
        "  ssrew.append(ssr)\n",
        "  dones.append(done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ALggHOpUrpJK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "ALggHOpUrpJK",
        "outputId": "8fd419e8-b3e7-40f3-b8e0-06a53fbc83da"
      },
      "outputs": [],
      "source": [
        "x = range(len(ssrew))\n",
        "plt.plot(x,ssrew)\n",
        "plt.title('Total Reward per Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609edd6b",
      "metadata": {},
      "source": [
        "**Qlearning VS SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bf74ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x,epsireward)\n",
        "plt.plot(x,sdrew)\n",
        "plt.title('Qlearning vs SARSA (Deterministic)')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "plt.legend(['Qlearning', 'SARSA'], loc='upper left')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x,sqrewards)\n",
        "plt.plot(x,ssrew)\n",
        "plt.title('Qlearning vs SARSA (Stochastic')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "plt.legend(['Qlearning', 'SARSA'], loc='upper left')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
